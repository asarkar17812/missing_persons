{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39a934a",
   "metadata": {},
   "source": [
    "# Scaling of Missing Persons across North America, Mexico, and South America:\n",
    "---\n",
    "### Ayush Sarkar | Missing Persons w/ Dynamical Systems Lab @ NYU\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e7767",
   "metadata": {},
   "source": [
    "# Introduction: \n",
    "* To better understand the parallels in the number of missing persons cases in urban areas, there was an interest in investigating the scaling rates between two different countries and comparing them. \n",
    "* We considered many different South American countries, including Mexico, Columbia, Brazil, and Argentina. Despite considering so many other countries, Mexico had the most complete and similar administrative divisional structure to the United States, although Colombia did seem promising.\n",
    "\n",
    "# Methodology: \n",
    "* We consider three main datasets in regards to developing the United States CBSA case sums and population estimates for scaling: \n",
    "  1) NamUs Missing Persons Dataset (scraped as of 07/17/2025)\n",
    "  2) SEER Age-adjusted Population Estimates 1969-2022 \n",
    "  3) QCEW-County-MSA-CSA Crosswalk (Contains historical crosswalks from 1990 - Dec. 2003, Dec. 2003 - Feb. 2013, Feb. 2013 - July 2023, July 2023 - Present)\n",
    "*    \n",
    "\n",
    "* From investigation, it seems as if there are few public missing persons databases available due to the sensitive nature of these cases and the family members involved. There is seemingly a federal-level database, although it is not accessible to public under normal circumstances. Thus, we consider the National Missing and Unidentified Persons System, or NamUs for short, as which was scraped as of 07/17/2025 to generate the supplemental json file included. \n",
    "  * The NamUs database contained about 25,630 entries as of 07/17/2025.\n",
    "  * This cases include entries from 1902-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b2c22d",
   "metadata": {},
   "source": [
    "# Code: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e232b7e3",
   "metadata": {},
   "source": [
    "## Data Cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa982ff",
   "metadata": {},
   "source": [
    "### United States NamUs Cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e86bc4b",
   "metadata": {},
   "source": [
    "#### SEER Population Estimate Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b56313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# ============================================================\n",
    "# STEP 0: SEER Historical County Population Estimates Processing\n",
    "# ============================================================\n",
    "\n",
    "def clean_and_export_population_data(input_file, output_csv_file):\n",
    "    cleaned_data = []\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) < 18:\n",
    "                continue\n",
    "            try:\n",
    "                cleaned_data.append({\n",
    "                    'Year': int(line[0:4]),\n",
    "                    'FIPS': line[6:11],\n",
    "                    'Population': int(line[18:]) if line[18:].isdigit() else None\n",
    "                })\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    if cleaned_data:\n",
    "        with open(output_csv_file, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=cleaned_data[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(cleaned_data)\n",
    "\n",
    "\n",
    "clean_and_export_population_data(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\source\\SEER Population Estimates\\us_1969_2022.19ages.adjusted.txt',\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\us_pop_by_decade.csv'\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: Load Inputs\n",
    "# ============================================================\n",
    "\n",
    "df_population = pd.read_csv(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\us_pop_by_decade.csv',\n",
    "    dtype={'Year': int, 'FIPS': str}\n",
    ")\n",
    "\n",
    "df_cencount = pd.read_csv(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\source\\NBER County Population Estimates\\cencounts.csv',\n",
    "    dtype=str\n",
    ")\n",
    "\n",
    "df_pop_est = pd.read_csv(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\source\\2024 County Population Est\\co-est2024-alldata.csv',\n",
    "    dtype={'STATE': str, 'COUNTY': str},\n",
    "    encoding='latin1'\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: Aggregate SEER + Append 2023–2024\n",
    "# ============================================================\n",
    "\n",
    "df_population = (\n",
    "    df_population\n",
    "    .groupby(['FIPS', 'Year'], as_index=False)\n",
    "    .agg({'Population': 'sum'})\n",
    ")\n",
    "\n",
    "df_pop_est['FIPS'] = df_pop_est['STATE'] + df_pop_est['COUNTY']\n",
    "df_pop_est = df_pop_est[~df_pop_est['FIPS'].str.endswith('000')]\n",
    "\n",
    "df_2023 = df_pop_est[['FIPS', 'POPESTIMATE2023']].rename(\n",
    "    columns={'POPESTIMATE2023': 'Population'}\n",
    ")\n",
    "df_2023['Year'] = 2023\n",
    "\n",
    "df_2024 = df_pop_est[['FIPS', 'POPESTIMATE2024']].rename(\n",
    "    columns={'POPESTIMATE2024': 'Population'}\n",
    ")\n",
    "df_2024['Year'] = 2024\n",
    "\n",
    "df_population = pd.concat([df_population, df_2023, df_2024], ignore_index=True)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: Normalize FIPS (NO corrections here)\n",
    "# ============================================================\n",
    "\n",
    "df_population['FIPS'] = df_population['FIPS'].astype(str).str.zfill(5)\n",
    "df_population = df_population[\n",
    "    ~df_population['FIPS'].str.match(r'^\\d{2}9\\d{2}$')\n",
    "]\n",
    "\n",
    "df_cencount['fips'] = df_cencount['fips'].astype(str).str.zfill(5)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: Apply FIPS Corrections TO df_cencount (✅ FIX)\n",
    "# ============================================================\n",
    "\n",
    "fips_corrections = {\n",
    "    '12025': '12086',  # Miami-Dade → Dade\n",
    "}\n",
    "\n",
    "df_cencount['fips_corrected'] = df_cencount['fips'].replace(fips_corrections)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5: Authoritative Merge\n",
    "# ============================================================\n",
    "\n",
    "df_merged = df_population.merge(\n",
    "    df_cencount[['fips_corrected', 'name']],\n",
    "    left_on='FIPS',\n",
    "    right_on='fips_corrected',\n",
    "    how='left'\n",
    ").drop(columns='fips_corrected')\n",
    "\n",
    "df_merged['source'] = np.where(df_merged['name'].notna(), 'table', None)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 6: Shapefile Fallback (only unresolved)\n",
    "# ============================================================\n",
    "\n",
    "df_nan = df_merged[df_merged['name'].isna()].copy()\n",
    "df_nan['name_filled'] = None\n",
    "df_nan['source'] = None\n",
    "\n",
    "def build_fips_map(gdf):\n",
    "    if 'GEOID' in gdf.columns:\n",
    "        fips_col = 'GEOID'\n",
    "    elif 'STATEFP' in gdf.columns and 'COUNTYFP' in gdf.columns:\n",
    "        gdf['FIPS'] = gdf['STATEFP'].astype(str).str.zfill(2) + gdf['COUNTYFP'].astype(str).str.zfill(3)\n",
    "        fips_col = 'FIPS'\n",
    "    elif 'CNTY_FIPS' in gdf.columns:\n",
    "        gdf['FIPS'] = gdf['CNTY_FIPS'].astype(str).str.zfill(5)\n",
    "        fips_col = 'FIPS'\n",
    "    else:\n",
    "        raise ValueError(\"No recognizable FIPS columns found.\")\n",
    "\n",
    "    for name_col in ['NAMELSAD', 'NAME', 'COUNTYNAME']:\n",
    "        if name_col in gdf.columns:\n",
    "            gdf[name_col] = gdf[name_col].astype(str).str.strip()\n",
    "            return dict(zip(gdf[fips_col].astype(str).str.zfill(5), gdf[name_col]))\n",
    "\n",
    "    raise ValueError(\"No recognizable county name column found.\")\n",
    "\n",
    "county_shape_files = {\n",
    "    2024: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\2024\\counties\\tl_2024_us_county.shp',\n",
    "    2023: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\2023\\US_county_2023.shp',\n",
    "    2022: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\2022\\US_county_2022.shp',\n",
    "    2010: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\2010\\US_county_2010.shp',\n",
    "    2000: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\2000\\US_county_2000.shp',\n",
    "    1990: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\1990\\US_county_1990.shp',\n",
    "    1980: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\1980\\US_county_1980.shp',\n",
    "    1970: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\1970\\US_county_1970_conflated.shp',\n",
    "    1960: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\1960\\US_county_1960_conflated.shp',\n",
    "    1950: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\1950\\US_county_1950_conflated.shp',\n",
    "    1940: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\1940\\US_county_1940_conflated.shp',\n",
    "    1930: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\1930\\US_county_1930_conflated.shp',\n",
    "    1920: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\1920\\US_county_1920_conflated.shp',\n",
    "    1910: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\1910\\US_county_1910_conflated.shp',\n",
    "    1900: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\1900\\US_county_1900_conflated.shp'\n",
    "}\n",
    "subdivision_shape_files = {\n",
    "    2023: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\2023\\subdivisions\\US_cty_sub_2023.shp',\n",
    "    2022: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\2022\\subdivisons\\US_cty_sub_2022.shp',\n",
    "    2010: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\2010\\subdivisions\\US_cty_sub_2010.shp',\n",
    "    2000: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\2000\\subdivisions\\US_cty_sub_2000.shp',\n",
    "    1990: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\1990\\subdivisions\\US_cty_sub_1990.shp',\n",
    "    1980: r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\shape files\\1980\\subdivisions\\US_mcd_1980.shp'\n",
    "}\n",
    "\n",
    "for year, path in county_shape_files.items():\n",
    "    gdf = gpd.read_file(path)\n",
    "    fips_map = build_fips_map(gdf)\n",
    "\n",
    "    mask = df_nan['name_filled'].isna()\n",
    "    matches = df_nan.loc[mask, 'FIPS'].map(fips_map)\n",
    "\n",
    "    df_nan.loc[mask, 'name_filled'] = matches\n",
    "    df_nan.loc[mask & matches.notna(), 'source'] = f'shapefile_{year}'\n",
    "\n",
    "for year, path in subdivision_shape_files.items():\n",
    "    gdf = gpd.read_file(path)\n",
    "    fips_map = build_fips_map(gdf)\n",
    "\n",
    "    mask = df_nan['name_filled'].isna()\n",
    "    matches = df_nan.loc[mask, 'FIPS'].map(fips_map)\n",
    "\n",
    "    df_nan.loc[mask, 'name_filled'] = matches\n",
    "    df_nan.loc[mask & matches.notna(), 'source'] = f'shapefile_{year}'\n",
    "\n",
    "df_nan['name'] = df_nan['name_filled']\n",
    "df_merged.update(df_nan[['FIPS', 'name', 'source']])\n",
    "\n",
    "df_merged['State'] = df_merged['name'].str.extract(r'^([A-Z]{2})\\s+')\n",
    "\n",
    "us_state_abbrev = {\n",
    "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
    "    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
    "    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas',\n",
    "    'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland', 'MA': 'Massachusetts',\n",
    "    'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri', 'MT': 'Montana',\n",
    "    'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico',\n",
    "    'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma',\n",
    "    'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n",
    "    'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming',\n",
    "    'DC': 'District of Columbia'\n",
    "}\n",
    "\n",
    "state_fips_to_abbr = {\n",
    "    '01':'AL','02':'AK','04':'AZ','05':'AR','06':'CA','08':'CO','09':'CT','10':'DE',\n",
    "    '11':'DC','12':'FL','13':'GA','15':'HI','16':'ID','17':'IL','18':'IN','19':'IA',\n",
    "    '20':'KS','21':'KY','22':'LA','23':'ME','24':'MD','25':'MA','26':'MI','27':'MN',\n",
    "    '28':'MS','29':'MO','30':'MT','31':'NE','32':'NV','33':'NH','34':'NJ','35':'NM',\n",
    "    '36':'NY','37':'NC','38':'ND','39':'OH','40':'OK','41':'OR','42':'PA','44':'RI',\n",
    "    '45':'SC','46':'SD','47':'TN','48':'TX','49':'UT','50':'VT','51':'VA','53':'WA',\n",
    "    '54':'WV','55':'WI','56':'WY'\n",
    "}\n",
    "\n",
    "df_merged['state_abbr'] = df_merged['FIPS'].str[:2].map(state_fips_to_abbr)\n",
    "df_merged['State'] = df_merged['state_abbr'].map(us_state_abbrev)\n",
    "df_merged = df_merged.drop(columns=['state_abbr'])\n",
    "df_merged['name'] = df_merged['name'].str.replace(r'^[A-Z]{2}\\s+', '', regex=True)\n",
    "\n",
    "df_merged.loc[(df_merged['name'] == 'Dade County') & (df_merged['State'] == 'Florida'), 'name'] = 'Miami-Dade County'\n",
    "df_merged.loc[(df_merged['name'] == 'La Salle County') & (df_merged['State'] == 'Illinois'), 'name'] = 'Lasalle County'\n",
    "df_merged.loc[(df_merged['name'] == 'DeBaca County') & (df_merged['State'] == 'New Mexico'), 'name'] = 'De Baca County'\n",
    "df_merged.loc[(df_merged['name'] == 'St. John the Baptist Par.') & (df_merged['State'] == 'Louisiana'), 'name'] = 'St. John the Baptist Parish'\n",
    "df_merged.loc[(df_merged['name'] == 'Dona Ana County') & (df_merged['State'] == 'New Mexico'), 'name'] = 'DOÑA ANA COUNTY'\n",
    "\n",
    "# ============================================================\n",
    "# STEP 7: Final Export\n",
    "# ============================================================\n",
    "\n",
    "df_merged.to_csv(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\population.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# print(\"✅ Export complete\")\n",
    "# print(df_merged['source'].value_counts(dropna=False))\n",
    "print(df_merged.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb68b88b",
   "metadata": {},
   "source": [
    "#### NamUs Scraping and Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# ===============================\n",
    "# Helper: tokenize missing values\n",
    "# ===============================\n",
    "def tokenize(value):\n",
    "    \"\"\"Convert empty/missing/unknown/redacted values into consistent tokens.\"\"\"\n",
    "    if value is None:\n",
    "        return \"MISSING\"\n",
    "    if isinstance(value, str):\n",
    "        stripped = value.strip().lower()\n",
    "        if stripped in [\"\", \"na\", \"n/a\", \"null\", \"not available\"]:\n",
    "            return \"CENSORED\"\n",
    "        if stripped in [\"unknown\", \"unk\"]:\n",
    "            return \"UNKNOWN\"\n",
    "    return value\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Load raw NamUs JSON\n",
    "# ===============================\n",
    "with open(\n",
    "    r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\output\\namus-20250717.json',\n",
    "    'r',\n",
    "    encoding='utf-8'\n",
    ") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "main_data = []\n",
    "\n",
    "for entry in data:\n",
    "    subject = entry.get(\"subjectIdentification\", {})\n",
    "    desc = entry.get(\"subjectDescription\", {})\n",
    "    physical = entry.get(\"physicalDescription\", {})\n",
    "    sighting = entry.get(\"sighting\", {})\n",
    "    agency = entry.get(\"primaryInvestigatingAgency\", {})\n",
    "\n",
    "    row = {\n",
    "        \"CaseID\": tokenize(entry.get(\"idFormatted\")),\n",
    "        \"CurrentMinAge\": tokenize(subject.get(\"currentMinAge\")),\n",
    "        \"CurrentMaxAge\": tokenize(subject.get(\"currentMaxAge\")),\n",
    "        \"Sex\": tokenize(desc.get(\"sex\", {}).get(\"name\") if desc.get(\"sex\") else None),\n",
    "        \"Ethnicity\": tokenize(desc.get(\"primaryEthnicity\", {}).get(\"name\") if desc.get(\"primaryEthnicity\") else None),\n",
    "        \"DisappearanceDate\": tokenize(sighting.get(\"date\")),\n",
    "        \"City\": tokenize(sighting.get(\"address\", {}).get(\"city\") if sighting.get(\"address\") else None),\n",
    "        \"State\": tokenize(\n",
    "            sighting.get(\"address\", {})\n",
    "            .get(\"state\", {})\n",
    "            .get(\"name\") if sighting.get(\"address\") else None\n",
    "        ),\n",
    "        \"County\": tokenize(\n",
    "            sighting.get(\"address\", {})\n",
    "            .get(\"county\", {})\n",
    "            .get(\"name\") if sighting.get(\"address\") else None\n",
    "        ),\n",
    "        \"InvestigatingAgency\": tokenize(agency.get(\"name\")),\n",
    "    }\n",
    "\n",
    "    main_data.append(row)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Write intermediate CSV\n",
    "# ===============================\n",
    "output_csv = r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\cleaned_missing_persons.csv'\n",
    "\n",
    "with open(output_csv, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=main_data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(main_data)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Reload as DataFrame\n",
    "# ===============================\n",
    "df_namus = pd.read_csv(\n",
    "    output_csv,\n",
    "    parse_dates=['DisappearanceDate'],\n",
    "    date_parser=lambda x: pd.to_datetime(x, errors='coerce')\n",
    ")\n",
    "\n",
    "df_namus = df_namus[\n",
    "    [\"CaseID\", \"CurrentMinAge\", \"CurrentMaxAge\", \"Sex\", \"Ethnicity\",\n",
    "     \"DisappearanceDate\", \"City\", \"State\", \"County\"]\n",
    "].copy()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Year handling (cap pre-1969)\n",
    "# ===============================\n",
    "df_namus['Year'] = df_namus['DisappearanceDate'].dt.year\n",
    "df_namus.loc[df_namus['Year'] < 1969, 'Year'] = 1969\n",
    "df_namus.loc[df_namus['Year'] > 2024, 'Year'] = 2024\n",
    "df_namus['Year'] = df_namus['Year'].astype(int)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Connecticut post-2022 handling\n",
    "# ===============================\n",
    "connecticut_cities_to_county = {\n",
    "    'EAST HARTFORD': 'CAPITOL PLANNING REGION',\n",
    "    'MERIDEN': 'SOUTH CENTRAL CONNECTICUT PLANNING REGION',\n",
    "    'NEW BRITAIN': 'CAPITOL PLANNING REGION',\n",
    "    'TORRINGTON': 'NORTHWEST HILLS PLANNING REGION',\n",
    "    'WEST HARTFORD': 'CAPITOL PLANNING REGION',\n",
    "    'GLASTONBURY': 'CAPITOL PLANNING REGION',\n",
    "    'DERBY': 'NAUGATUCK VALLEY PLANNING REGION',\n",
    "    'LISBON': 'SOUTHEASTERN CONNECTICUT PLANNING REGION',\n",
    "    'AVON': 'CAPITOL PLANNING REGION',\n",
    "    'GUILFORD': 'SOUTH CENTRAL CONNECTICUT PLANNING REGION',\n",
    "    'HAMDEN': 'SOUTH CENTRAL CONNECTICUT PLANNING REGION',\n",
    "    'GROTON': 'SOUTHEASTERN CONNECTICUT PLANNING REGION',\n",
    "    'BRIDGEPORT': 'GREATER BRIDGEPORT PLANNING REGION',\n",
    "    'NEW HAVEN': 'SOUTH CENTRAL CONNECTICUT PLANNING REGION',\n",
    "    'HARTFORD': 'CAPITOL PLANNING REGION',\n",
    "    'LEDYARD': 'SOUTHEASTERN CONNECTICUT PLANNING REGION',\n",
    "    'DANBURY': 'SOUTHEASTERN CONNECTICUT PLANNING REGION'\n",
    "}\n",
    "\n",
    "# Normalize early\n",
    "df_namus['State'] = df_namus['State'].astype(str).str.strip().str.upper()\n",
    "df_namus['County'] = df_namus['County'].astype(str).str.strip().str.upper()\n",
    "df_namus['City'] = df_namus['City'].astype(str).str.strip().str.upper()\n",
    "\n",
    "ct_mask = (df_namus['State'] == 'CONNECTICUT') & (df_namus['Year'] > 2022)\n",
    "mapped_ct = df_namus.loc[ct_mask, 'City'].map(connecticut_cities_to_county)\n",
    "df_namus.loc[ct_mask, 'County'] = mapped_ct.combine_first(df_namus.loc[ct_mask, 'County'])\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Drop territories\n",
    "# \n",
    "# ===============================\n",
    "dropped_states = {\n",
    "    'PUERTO RICO',\n",
    "    'VIRGIN ISLANDS',\n",
    "    'GUAM',\n",
    "    'NORTHERN MARIANA ISLANDS'\n",
    "}\n",
    "df_namus = df_namus[~df_namus['State'].isin(dropped_states)]\n",
    "\n",
    "# ===============================\n",
    "# Bad county flag\n",
    "# ===============================\n",
    "bad_values = {'MISSING', 'UNKNOWN', 'CENSORED'}\n",
    "\n",
    "df_namus['County'] = (\n",
    "    df_namus['County']\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .replace({'NAN': np.nan})\n",
    ")\n",
    "\n",
    "df_namus = df_namus[\n",
    "    df_namus['County'].notna() &\n",
    "    (~df_namus['County'].isin(bad_values))\n",
    "].copy()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Export final NamUs cases\n",
    "# Total Cases: 25532\n",
    "# ===============================\n",
    "df_namus.to_csv( r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\namus_cases.csv', index=False)\n",
    "\n",
    "print(\"Final row count:\", len(df_namus))\n",
    "print(df_namus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287abc87",
   "metadata": {},
   "source": [
    "#### Crosswalk Cleaning and Merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- Load files ---\n",
    "df_population = pd.read_csv(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\population.csv',\n",
    "    dtype={'FIPS': str}\n",
    ")\n",
    "df_namus = pd.read_csv(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\namus_cases.csv'\n",
    ")\n",
    "crosswalk_file = r'F:\\dsl_CLIMA\\projects\\Missing Persons Project\\working_dfs\\qcew-county-msa-csa-crosswalk.xlsx'\n",
    "\n",
    "# --- Helper functions ---\n",
    "bad_values = {'MISSING', 'UNKNOWN', 'CENSORED'}\n",
    "def clean_crosswalk(df_cw):\n",
    "    df_cw[['County Name', 'State Full']] = df_cw['County Title'].str.upper().str.split(',', n=1, expand=True)\n",
    "    df_cw['County Name'] = df_cw['County Name'].str.strip()\n",
    "    df_cw['State Full'] = df_cw['State Full'].str.strip()\n",
    "    \n",
    "    msa_split = df_cw['MSA Title'].str.upper().str.split(',', n=1, expand=True)\n",
    "    df_cw['MSA Name'] = msa_split[0].str.strip()\n",
    "    df_cw['MSA State Abbr'] = msa_split[1].str.strip().str.slice(0, 2)\n",
    "    \n",
    "    df_cw['County Code'] = df_cw['County Code'].astype(str).str.zfill(5)\n",
    "    df_cw['MSA Code'] = df_cw['MSA Code'].astype(str).str.zfill(5)\n",
    "    \n",
    "    return df_cw\n",
    "\n",
    "def merge_pop_with_crosswalk(df_subset, cw, bad_values={'MISSING', 'UNKNOWN', 'CENSORED'}):\n",
    "    df = df_subset.copy()\n",
    "\n",
    "    # --- Normalize columns ---\n",
    "    df['County_norm'] = df['County'].astype(str).str.upper().str.strip()\n",
    "    df['State_norm'] = df['State'].astype(str).str.upper().str.strip()\n",
    "\n",
    "    cw['County_norm'] = cw['County Title'].astype(str).str.upper().str.strip()\n",
    "    cw['MSA_Title_norm'] = cw['MSA Title'].astype(str).str.upper().str.split(',', n=1).str[0].str.strip()\n",
    "    cw['State_abbr'] = cw['MSA Title'].astype(str).str.extract(r',\\s*([^\\s]+)', expand=False)\n",
    "    \n",
    "    # Map abbreviation to full state name if needed\n",
    "    us_state_abbrev = {\n",
    "        'AL': 'Alabama','AK': 'Alaska','AZ': 'Arizona','AR': 'Arkansas','CA': 'California',\n",
    "        'CO': 'Colorado','CT': 'Connecticut','DE': 'Delaware','FL': 'Florida','GA': 'Georgia',\n",
    "        'HI': 'Hawaii','ID': 'Idaho','IL': 'Illinois','IN': 'Indiana','IA': 'Iowa','KS': 'Kansas',\n",
    "        'KY': 'Kentucky','LA': 'Louisiana','ME': 'Maine','MD': 'Maryland','MA': 'Massachusetts',\n",
    "        'MI': 'Michigan','MN': 'Minnesota','MS': 'Mississippi','MO': 'Missouri','MT': 'Montana',\n",
    "        'NE': 'Nebraska','NV': 'Nevada','NH': 'New Hampshire','NJ': 'New Jersey','NM': 'New Mexico',\n",
    "        'NY': 'New York','NC': 'North Carolina','ND': 'North Dakota','OH': 'Ohio','OK': 'Oklahoma',\n",
    "        'OR': 'Oregon','PA': 'Pennsylvania','RI': 'Rhode Island','SC': 'South Carolina',\n",
    "        'SD': 'South Dakota','TN': 'Tennessee','TX': 'Texas','UT': 'Utah','VT': 'Vermont',\n",
    "        'VA': 'Virginia','WA': 'Washington','WV': 'West Virginia','WI': 'Wisconsin','WY': 'Wyoming',\n",
    "        'D.C.': 'District of Columbia'\n",
    "    }\n",
    "    cw['State_full'] = cw['State_abbr'].map(us_state_abbrev)\n",
    "\n",
    "    # --- Split good vs bad counties ---\n",
    "    good_mask = df['County'].notna() & (~df['County'].isin(bad_values))\n",
    "    df_good = df[good_mask].copy()\n",
    "\n",
    "    # --- Merge good counties by FIPS ---\n",
    "    df_good = df_good.merge(\n",
    "        cw[['County Code','County Title','MSA Code','CSA Code','MSA Title','CSA Title']],\n",
    "        left_on='FIPS',\n",
    "        right_on='County Code',\n",
    "        how='left'\n",
    "    ).drop(columns=['County Code'], errors='ignore')\n",
    "\n",
    "    return df_good\n",
    "\n",
    "def merge_cases_with_crosswalk(df_subset, cw, bad_values={'MISSING', 'UNKNOWN', 'CENSORED'}):\n",
    "    df = df_subset.copy()\n",
    "\n",
    "    # --- Normalize columns ---\n",
    "    df['County_norm'] = df['County'].astype(str).str.upper().str.strip()\n",
    "    df['City_norm'] = df['City'].astype(str).str.upper().str.strip() \n",
    "    df['State_norm'] = df['State'].astype(str).str.upper().str.strip()\n",
    "\n",
    "    cw['County_norm'] = cw['County Title'].astype(str).str.upper().str.strip()\n",
    "    cw['MSA_Title_norm'] = cw['MSA Title'].astype(str).str.upper().str.split(',', n=1).str[0].str.strip()\n",
    "    cw['State_abbr'] = cw['MSA Title'].astype(str).str.extract(r',\\s*([^\\s]+)', expand=False)\n",
    "    \n",
    "    # Map abbreviation to full state name if needed\n",
    "    us_state_abbrev = {\n",
    "        'AL': 'Alabama','AK': 'Alaska','AZ': 'Arizona','AR': 'Arkansas','CA': 'California',\n",
    "        'CO': 'Colorado','CT': 'Connecticut','DE': 'Delaware','FL': 'Florida','GA': 'Georgia',\n",
    "        'HI': 'Hawaii','ID': 'Idaho','IL': 'Illinois','IN': 'Indiana','IA': 'Iowa','KS': 'Kansas',\n",
    "        'KY': 'Kentucky','LA': 'Louisiana','ME': 'Maine','MD': 'Maryland','MA': 'Massachusetts',\n",
    "        'MI': 'Michigan','MN': 'Minnesota','MS': 'Mississippi','MO': 'Missouri','MT': 'Montana',\n",
    "        'NE': 'Nebraska','NV': 'Nevada','NH': 'New Hampshire','NJ': 'New Jersey','NM': 'New Mexico',\n",
    "        'NY': 'New York','NC': 'North Carolina','ND': 'North Dakota','OH': 'Ohio','OK': 'Oklahoma',\n",
    "        'OR': 'Oregon','PA': 'Pennsylvania','RI': 'Rhode Island','SC': 'South Carolina',\n",
    "        'SD': 'South Dakota','TN': 'Tennessee','TX': 'Texas','UT': 'Utah','VT': 'Vermont',\n",
    "        'VA': 'Virginia','WA': 'Washington','WV': 'West Virginia','WI': 'Wisconsin','WY': 'Wyoming',\n",
    "        'D.C.': 'District of Columbia'\n",
    "    }\n",
    "    cw['State_full'] = cw['State_abbr'].map(us_state_abbrev)\n",
    "\n",
    "    # --- Split good vs bad counties ---\n",
    "    good_mask = df['County'].notna() & (~df['County'].isin(bad_values))\n",
    "    df_good = df[good_mask].copy()\n",
    "    df_bad = df[~good_mask].copy()\n",
    "\n",
    "    # --- Merge good counties by FIPS ---\n",
    "    df_good = df_good.merge(\n",
    "        cw[['County Code','County Title','MSA Code','CSA Code','MSA Title','CSA Title']],\n",
    "        left_on='FIPS',\n",
    "        right_on='County Code',\n",
    "        how='left'\n",
    "    ).drop(columns=['County Code'], errors='ignore')\n",
    "\n",
    "    # --- Merge bad counties by City → MSA ---\n",
    "    df_bad = df_bad.merge(\n",
    "        cw[['MSA_Title_norm','State_full','MSA Code','CSA Code','MSA Title','CSA Title']],\n",
    "        left_on=['City_norm','State_norm'],\n",
    "        right_on=['MSA_Title_norm','State_full'],\n",
    "        how='left'\n",
    "    ).drop(columns=['MSA_Title_norm','State_full'], errors='ignore')\n",
    "\n",
    "    # --- Fill FIPS and County for MSAs with a single county ---\n",
    "    msa_single = (\n",
    "        cw.groupby('MSA Code', as_index=False)\n",
    "        .agg({'County Code':'nunique','County Title':'first'})\n",
    "        .query('`County Code` == 1')\n",
    "        .rename(columns={'County Title':'Single_County_Title'})\n",
    "    )\n",
    "    msa_code_fill = cw.groupby('MSA Code', as_index=False).agg({'County Code':'first'})\n",
    "    msa_single = msa_single.drop(columns=['County Code']).merge(msa_code_fill, on='MSA Code')\n",
    "\n",
    "    df_bad = df_bad.merge(msa_single, on='MSA Code', how='left')\n",
    "    df_bad['FIPS'] = df_bad['FIPS'].fillna(df_bad['County Code'])\n",
    "    df_bad['County'] = df_bad['County'].fillna(df_bad['Single_County_Title'])\n",
    "    df_bad.drop(columns=['County Code','Single_County_Title'], inplace=True)\n",
    "\n",
    "    # --- Recombine ---\n",
    "    df_merged = pd.concat([df_good, df_bad], ignore_index=True)\n",
    "    df_merged.drop(columns=['County_norm','City_norm','State_norm'], errors='ignore', inplace=True)\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "def summarize_population_by_msa_all_years(df):\n",
    "    return (\n",
    "        df.groupby(['Year', 'MSA Code'], as_index=False)\n",
    "          .agg(MSA_pop=('Population', 'sum'))\n",
    "          .sort_values(['Year', 'MSA Code'])\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "def summarize_population_by_csa_all_years(df):\n",
    "    return (\n",
    "        df.groupby(['Year', 'CSA Code'], as_index=False)\n",
    "          .agg(CSA_pop=('Population', 'sum'))\n",
    "          .sort_values(['Year', 'CSA Code'])\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "def simplify_titles(df):\n",
    "    if 'MSA Title' in df.columns:\n",
    "        df['CBSA Type'] = df['MSA Title'].astype(str).str.extract(r'(\\w+)$')[0].replace('nan', np.nan)\n",
    "        df['MSA Title'] = df['MSA Title'].astype(str).str.split(',', n=1).str[0].str.strip()\n",
    "    if 'CSA Title' in df.columns:\n",
    "        df['CSA Type'] = df['CSA Title'].astype(str).str.extract(r'(\\w+)$')[0].replace('nan', np.nan)\n",
    "        df['CSA Title'] = df['CSA Title'].astype(str).str.split(',', n=1).str[0].str.strip()\n",
    "    return df\n",
    "\n",
    "# --- Normalize for merging ---\n",
    "df_namus['County_norm'] = df_namus['County'].str.upper().str.strip()\n",
    "df_namus['State_norm'] = df_namus['State'].str.upper().str.strip()\n",
    "df_population['County_norm'] = df_population['name'].str.upper().str.strip()\n",
    "df_population['State_norm'] = df_population['State'].str.upper().str.strip()\n",
    "df_namus['Year'] = df_namus['Year'].astype(int)\n",
    "df_population['Year'] = df_population['Year'].astype(int)\n",
    "\n",
    "# --- Deduplicate population to avoid row multiplication ---\n",
    "df_population = df_population.drop_duplicates(subset=['Year', 'State_norm', 'County_norm'])\n",
    "\n",
    "# --- Load and clean crosswalks ---\n",
    "cw_2003 = clean_crosswalk(pd.read_excel(crosswalk_file, sheet_name='Dec. 2003 Crosswalk', dtype=str))\n",
    "cw_2013 = clean_crosswalk(pd.read_excel(crosswalk_file, sheet_name='Feb. 2013 Crosswalk', dtype=str))\n",
    "cw_2023 = clean_crosswalk(pd.read_excel(crosswalk_file, sheet_name='Jul. 2023 Crosswalk', dtype=str))\n",
    "\n",
    "# --- Split Population by year ---\n",
    "df_population['County'] = df_population['name'].copy()\n",
    "df_pop_2003 = df_population[df_population['Year'] <= 2003]\n",
    "df_pop_2013 = df_population[(df_population['Year'] > 2003) & (df_population['Year'] < 2013)]\n",
    "df_pop_2023 = df_population[df_population['Year'] >= 2013]\n",
    "\n",
    "df_pop_final = pd.concat([\n",
    "    merge_pop_with_crosswalk(df_pop_2003, cw_2003),\n",
    "    merge_pop_with_crosswalk(df_pop_2013, cw_2013),\n",
    "    merge_pop_with_crosswalk(df_pop_2023, cw_2023)\n",
    "], ignore_index=True)\n",
    "\n",
    "df_cbsa = summarize_population_by_msa_all_years(df_pop_final)\n",
    "df_csa = summarize_population_by_csa_all_years(df_pop_final)\n",
    "\n",
    "# --- Summarize populations ---\n",
    "df_pop_final = (\n",
    "    df_pop_final\n",
    "    .merge(df_cbsa, on=['Year', 'MSA Code'], how='left')\n",
    "    .merge(df_csa, on=['Year', 'CSA Code'], how='left')\n",
    "    .rename(columns={'Population': 'County_pop'})\n",
    ").copy()\n",
    "\n",
    "df_pop_final = simplify_titles(df_pop_final)\n",
    "\n",
    "# --- Merge population ---\n",
    "df_namus = df_namus.merge(\n",
    "    df_pop_final[['FIPS', 'Year', 'County_pop', 'name', 'source', 'State', 'MSA Code', 'CSA Code', 'MSA Title', 'CSA Title', 'MSA_pop', 'CSA_pop', 'CBSA Type', 'CSA Type', 'County_norm', 'State_norm']],\n",
    "    on=['Year', 'County_norm', 'State_norm'],\n",
    "    how='left'\n",
    ").drop_duplicates()\n",
    "\n",
    "df_namus = df_namus[['CaseID','CurrentMinAge','CurrentMaxAge','Sex','Ethnicity','DisappearanceDate','City','State_x','County','Year','FIPS','County_pop','MSA Code','CSA Code','MSA Title','CSA Title','MSA_pop','CSA_pop','CBSA Type','CSA Type']]\n",
    "df_namus = df_namus.rename(columns={'State_x': 'State'})\n",
    "# --- Filter years and drop territories ---\n",
    "# df_namus = df_namus[(df_namus['Year'] > 1999) & (df_namus['Year'] < 2025)]\n",
    "\n",
    "# # --- Drop rows without FIPS after merge ---\n",
    "df_namus = df_namus[df_namus['FIPS'].notna()].copy()\n",
    "\n",
    "# --- Export ---\n",
    "df_namus.to_csv(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\mp_term.csv', index=False)\n",
    "\n",
    "df_pop_final = df_pop_final[['FIPS', 'Year', 'County_pop', 'name', 'source', 'State', 'MSA Code', 'CSA Code', 'MSA Title', 'CSA Title', 'MSA_pop', 'CSA_pop', 'CBSA Type', 'CSA Type']]\n",
    "df_pop_final.to_csv(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\pop_term.csv', index=False)\n",
    "\n",
    "print(\"Final row count:\", len(df_namus))\n",
    "print(df_namus.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9381789",
   "metadata": {},
   "source": [
    "### Mexico INEGI Cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc660bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "from datetime import datetime\n",
    "\n",
    "df_inegi = pd.read_csv(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\source\\mexico_missing_persons\\data.csv', dtype=str)\n",
    "\n",
    "def print_unknown_confidential_counts(df):\n",
    "    for col in df.columns:\n",
    "        count = df[col].isin(['UNKNOWN', 'CONFIDENTIAL', 'MISSING']).sum()\n",
    "        total = df[col].size\n",
    "        percent = (count / total) * 100 if total > 0 else 0\n",
    "        print(f\"{col}: (Count: {count}, Total: {total}); {percent:.2f}%\")\n",
    "\n",
    "print_unknown_confidential_counts(df_inegi)\n",
    "# VICTIM_ID: (Count: 0, Total: 129830); 0.00%\n",
    "# ORIGIN_AGENCY: (Count: 0, Total: 129830); 0.00%\n",
    "# DATE_OF_BIRTH: (Count: 76997, Total: 129830); 59.31%\n",
    "# SEX: (Count: 48105, Total: 129830); 37.05%\n",
    "# DATE_OF_INCIDENCE: (Count: 55991, Total: 129830); 43.13%\n",
    "# DATE_OF_REPORT: (Count: 53904, Total: 129830); 41.52%\n",
    "# VICTIM_STATUS: (Count: 124808, Total: 129830); 96.13%\n",
    "# STATE_ID: (Count: 0, Total: 129830); 0.00%\n",
    "# STATE: (Count: 2936, Total: 129830); 2.26%\n",
    "# MUNICIPALITY_ID: (Count: 0, Total: 129830); 0.00%\n",
    "# MUNICIPALITY: (Count: 53132, Total: 129830); 40.92%\n",
    "\n",
    "def plot_missing_value_correlation(df, replace_special=True, special_values=None):\n",
    "\n",
    "    # Replace special placeholder strings with np.nan\n",
    "    if replace_special:\n",
    "        if special_values is None:\n",
    "            special_values = ['UNKNOWN']\n",
    "        df = df.replace(special_values, np.nan)\n",
    "\n",
    "    # Plot missing value correlation heatmap\n",
    "    msno.heatmap(df)\n",
    "    plt.title(\"Missing Value Correlation Heatmap\", fontsize=24)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\mexico\\missingValue_correlation_matrix.png', dpi=1200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_missing_value_correlation(df_inegi)\n",
    "\n",
    "df_temp = df_inegi.copy()\n",
    "df_temp = df_temp[df_temp['STATE'] == 'UNKNOWN']\n",
    "# print(df_temp)\n",
    "\n",
    "def plot_valid_entries_choropleth_shp(\n",
    "    df,\n",
    "    state_col='STATE',\n",
    "    columns_to_check=None,\n",
    "    special_values=None,\n",
    "    shapefile_path=None,\n",
    "    shapefile_state_col='name'\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a static choropleth map of Mexican states showing number of valid (non-missing) entries.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input dataset\n",
    "    - state_col (str): Column in df identifying states (e.g. 'STATE')\n",
    "    - columns_to_check (list): Columns to check for valid entries (non-'UNKNOWN')\n",
    "    - special_values (list): List of strings to treat as missing (default: ['UNKNOWN', 'CONFIDENTIAL'])\n",
    "    - shapefile_path (str): Path to the .shp file for Mexican state boundaries\n",
    "    - shapefile_state_col (str): Column in shapefile with state names to match against df[state_col]\n",
    "    \"\"\"\n",
    "\n",
    "    if shapefile_path is None:\n",
    "        raise ValueError(\"Please provide the path to a .shp file (shapefile_path).\")\n",
    "\n",
    "    if special_values is None:\n",
    "        special_values = ['UNKNOWN', 'CONFIDENTIAL']\n",
    "    if columns_to_check is None:\n",
    "        columns_to_check = [col for col in df.columns if col != state_col]\n",
    "\n",
    "    df = df.copy()\n",
    "    df[state_col] = df[state_col].astype(str).str.upper()\n",
    "\n",
    "    # Replace special values with NA\n",
    "    df[columns_to_check] = df[columns_to_check].replace(special_values, pd.NA)\n",
    "\n",
    "    # Count valid entries per state\n",
    "    valid_counts = df.groupby(state_col)[columns_to_check].apply(lambda g: g.notna().sum().sum()).reset_index()\n",
    "    valid_counts.columns = [state_col, 'valid_count']\n",
    "\n",
    "    # Load shapefile\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    gdf[shapefile_state_col] = gdf[shapefile_state_col].astype(str).str.upper()\n",
    "\n",
    "    # Merge with shapefile\n",
    "    merged = gdf.merge(valid_counts, left_on=shapefile_state_col, right_on=state_col, how='left')\n",
    "    merged['valid_count'] = merged['valid_count'].fillna(0)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "    merged.plot(column='valid_count',\n",
    "                ax=ax,\n",
    "                legend=True,\n",
    "                cmap='viridis',\n",
    "                edgecolor='black',\n",
    "                legend_kwds={'label': \"Valid Entry Count\", 'orientation': \"vertical\"})\n",
    "\n",
    "    ax.set_title(\"Valid Entries by Mexican State\", fontsize=15)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\mexico\\cases_choropleth.png', dpi=1200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_valid_entries_choropleth_shp(\n",
    "    df=df_inegi,\n",
    "    state_col='STATE',\n",
    "    columns_to_check=[\n",
    "        'DATE_OF_BIRTH',\n",
    "        'SEX',\n",
    "        'DATE_OF_INCIDENCE',\n",
    "        'DATE_OF_REPORT',\n",
    "        'VICTIM_STATUS'\n",
    "    ],\n",
    "    special_values=['UNKNOWN'],\n",
    "    shapefile_path=r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\source\\shape files\\mexico\\mexican-states.shp'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c58df9",
   "metadata": {},
   "source": [
    "# Data Visualization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ed649",
   "metadata": {},
   "source": [
    "## NamUs Visuals:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ea4672",
   "metadata": {},
   "source": [
    "### Scaling Plots:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4d988",
   "metadata": {},
   "source": [
    "##### Scaling of Cumulative Yearly NamUs Cases vs Population for Select Time Periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "df_primary = pd.read_csv(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\mp_term.csv')\n",
    "\n",
    "# --- Parse and clean the disappearance date\n",
    "df_primary['DisappearanceDate'] = pd.to_datetime(df_primary['DisappearanceDate'], errors='coerce')\n",
    "\n",
    "# --- Set date index and sort\n",
    "df_primary = df_primary.set_index('DisappearanceDate').sort_index()\n",
    "print(df_primary.isna().sum())\n",
    "\n",
    "# --- Resample: monthly counts from the full data (up to 2024)\n",
    "all_months = pd.date_range(start='2010-01-01', end='2024-12-31', freq='MS')\n",
    "monthly_counts = df_primary.resample('MS').size().reindex(all_months, fill_value=0)\n",
    "\n",
    "# --- Compute cumulative disappearances\n",
    "cumulative_disappearances = monthly_counts.cumsum()\n",
    "\n",
    "# --- Filter to plot only from 2000 onward\n",
    "plot_start = '2010-01-01'\n",
    "cumulative_to_plot = cumulative_disappearances[cumulative_disappearances.index >= plot_start]\n",
    "\n",
    "# --- January data points for markers (from 2000 onward)\n",
    "january_points = cumulative_to_plot[cumulative_to_plot.index.month == 1]\n",
    "\n",
    "# --- Plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.plot(cumulative_to_plot.index, cumulative_to_plot.values,\n",
    "         color='darkgreen', linewidth=2, label='Cumulative NamUS Missing Persons Cases (2010–2024)')\n",
    "\n",
    "\n",
    "# --- Formatting\n",
    "plt.title('Cumulative NamUS Missing Persons Cases by Month Start (2010–2024)', fontsize=24)\n",
    "plt.xlabel('Year', fontsize=20)\n",
    "plt.ylabel('Cumulative NamUS Missing Persons Cases', fontsize=28)\n",
    "plt.grid(False)\n",
    "\n",
    "# Format x-axis ticks for Januarys only\n",
    "jan_ticks = january_points.index\n",
    "plt.xticks(ticks=jan_ticks, labels=[date.strftime('%Y') for date in jan_ticks], fontsize=18, rotation=55)\n",
    "y_ticks=[0, 2000, 4000, 6000, 8000, 10000, 12000, 14000, 16000]\n",
    "plt.yticks(y_ticks, fontsize=14)\n",
    "plt.ylim(y_ticks[0] + .5)\n",
    "plt.xlim(all_months[0], all_months[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\regressions\\temporal\\[2010-2024]cumulative_cases.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df = df_primary.copy()\n",
    "\n",
    "# Full year range for cumulative data (1900 to 2024 inclusive)\n",
    "years = list(range(2010, 2025))  # 2025 excluded\n",
    "plot_start_year = 2010\n",
    "years_to_plot = []  # store years for which regression results will be plotted\n",
    "\n",
    "# Storage for regression results\n",
    "betas = []\n",
    "beta_err_lower = []\n",
    "beta_err_upper = []\n",
    "intercepts = []\n",
    "intercept_err_lower = []\n",
    "intercept_err_upper = []\n",
    "r2_values = []\n",
    "\n",
    "# New storage for total population and cumulative cases\n",
    "total_populations = []\n",
    "cumulative_cases = []\n",
    "\n",
    "for year in years:\n",
    "    # Use all cases up to current year (cumulative)\n",
    "    df_year = df[df['Year'] <= year]\n",
    "\n",
    "    grouped = (\n",
    "        df_year.groupby(['State', 'FIPS', 'County_pop'])\n",
    "        .agg(case_count=('CaseID', 'count'))\n",
    "        .reset_index()\n",
    "    )\n",
    "    grouped = grouped[grouped['case_count'] > 0]\n",
    "\n",
    "    if len(grouped) > 1:\n",
    "        X_log = np.log10(grouped['County_pop'].values)\n",
    "        y_log = np.log10(grouped['case_count'].values)\n",
    "        X_log_const = sm.add_constant(X_log)\n",
    "        model = sm.OLS(y_log, X_log_const).fit()\n",
    "\n",
    "        intercept = model.params[0]\n",
    "        beta = model.params[1]\n",
    "        r2 = model.rsquared\n",
    "\n",
    "        conf_int = model.conf_int()\n",
    "        intercept_ci_lower, intercept_ci_upper = conf_int[0, 0], conf_int[0, 1]\n",
    "        beta_ci_lower, beta_ci_upper = conf_int[1, 0], conf_int[1, 1]\n",
    "\n",
    "        # Calculate total population and cumulative cases for this year\n",
    "        total_pop = grouped['County_pop'].sum()\n",
    "        cum_cases = grouped['case_count'].sum()\n",
    "\n",
    "        if year >= plot_start_year:\n",
    "            intercepts.append(intercept)\n",
    "            intercept_err_lower.append(intercept - intercept_ci_lower)\n",
    "            intercept_err_upper.append(intercept_ci_upper - intercept)\n",
    "            betas.append(beta)\n",
    "            beta_err_lower.append(beta - beta_ci_lower)\n",
    "            beta_err_upper.append(beta_ci_upper - beta)\n",
    "            r2_values.append(r2)\n",
    "            total_populations.append(total_pop)\n",
    "            cumulative_cases.append(cum_cases)\n",
    "            years_to_plot.append(year)\n",
    "\n",
    "            print(\n",
    "                f\"{year}: Intercept = {intercept:.4f} [{intercept_ci_lower:.4f}, {intercept_ci_upper:.4f}], \"\n",
    "                f\"β = {beta:.4f} [{beta_ci_lower:.4f}, {beta_ci_upper:.4f}], R² = {r2:.4f}, \"\n",
    "                f\"Total Pop = {total_pop}, Cum Cases = {cum_cases}\"\n",
    "            )\n",
    "    else:\n",
    "        if year >= plot_start_year:\n",
    "            intercepts.append(np.nan)\n",
    "            intercept_err_lower.append(np.nan)\n",
    "            intercept_err_upper.append(np.nan)\n",
    "            betas.append(np.nan)\n",
    "            beta_err_lower.append(np.nan)\n",
    "            beta_err_upper.append(np.nan)\n",
    "            r2_values.append(np.nan)\n",
    "            total_populations.append(np.nan)\n",
    "            cumulative_cases.append(np.nan)\n",
    "            years_to_plot.append(year)\n",
    "            print(f\"{year}: insufficient data for regression\")\n",
    "\n",
    "# Plotting the cumulative scaling exponent β\n",
    "y_err = np.array([beta_err_lower, beta_err_upper])\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "plt.plot(years_to_plot, betas, color='lightseagreen', linewidth=2, label='β-value Estimate')\n",
    "plt.errorbar(\n",
    "    years_to_plot,\n",
    "    betas,\n",
    "    yerr=y_err,\n",
    "    fmt='o',\n",
    "    ecolor='k',\n",
    "    capsize=8,\n",
    "    capthick=2,\n",
    "    color='darkorange',\n",
    "    elinewidth=2,\n",
    "    label=r'$\\beta$ with 95% CI'\n",
    ")\n",
    "\n",
    "plt.title(r'Scaling Exponent ($\\beta$) of Total NamUS Missing Person Cases vs County Population (2010–2024)', fontsize=28)\n",
    "plt.xlabel('Cumulative NamUS Cases by Year', fontsize=24)\n",
    "plt.ylabel(r'Estimated Scaling Exponent Value ($\\beta$)', fontsize=24)\n",
    "plt.xticks(years_to_plot, rotation=65, fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend()\n",
    "max_beta = np.nanmax(betas)\n",
    "plt.ylim(0, max_beta + 0.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\regressions\\temporal\\[2010-2024]regression_ts_counties.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Identify years with best and worst R²\n",
    "r2_array = np.array(r2_values)\n",
    "valid_r2_indices = np.where(~np.isnan(r2_array))[0]\n",
    "best_year_idx = valid_r2_indices[np.argmax(r2_array[valid_r2_indices])]\n",
    "worst_year_idx = valid_r2_indices[np.argmin(r2_array[valid_r2_indices])]\n",
    "\n",
    "best_year = years_to_plot[best_year_idx]\n",
    "worst_year = years_to_plot[worst_year_idx]\n",
    "\n",
    "\n",
    "def plot_regression_scatter(ax, year, df, title_prefix=''):\n",
    "    # Use only cases that occurred within the specific year\n",
    "    df_year = df[df['Year'] == year]\n",
    "\n",
    "    # Group by spatial units and count cases for that year\n",
    "    grouped = (\n",
    "        df_year.groupby(['State', 'FIPS', 'County_pop'])\n",
    "        .agg(case_count=('CaseID', 'count'))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Filter valid rows\n",
    "    grouped = grouped[\n",
    "        (grouped['County_pop'] > 0) &\n",
    "        (grouped['case_count'] > 0)\n",
    "    ]\n",
    "\n",
    "    # Log-transform\n",
    "    grouped['log_pop'] = np.log10(grouped['County_pop'])\n",
    "    grouped['log_cases'] = np.log10(grouped['case_count'])\n",
    "\n",
    "    # Remove infs and NaNs\n",
    "    grouped = grouped[\n",
    "        np.isfinite(grouped['log_pop']) & np.isfinite(grouped['log_cases'])\n",
    "    ]\n",
    "\n",
    "    X = grouped['log_pop'].values\n",
    "    y = grouped['log_cases'].values\n",
    "\n",
    "    if len(X) < 2:\n",
    "        ax.set_title(f\"{title_prefix}{year}: Not enough data\", fontsize=16)\n",
    "        return\n",
    "\n",
    "    # Fit regression\n",
    "    X_const = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X_const).fit()\n",
    "    y_pred = model.predict(X_const)\n",
    "\n",
    "    # Extract stats\n",
    "    intercept = model.params[0]\n",
    "    beta = model.params[1]\n",
    "    conf_int = model.conf_int()\n",
    "    intercept_ci_lower, intercept_ci_upper = conf_int[0, 0], conf_int[0, 1]\n",
    "    beta_ci_lower, beta_ci_upper = conf_int[1, 0], conf_int[1, 1]\n",
    "\n",
    "    # Plot data and regression\n",
    "    ax.scatter(X, y, color='steelblue', alpha=0.7, label='Data Points')\n",
    "    ax.plot(X, y_pred, color='crimson', linewidth=2, label='Regression Line')\n",
    "\n",
    "    # Confidence interval shading\n",
    "    X_range = np.linspace(X.min(), X.max(), 200)\n",
    "    y_lower = intercept + beta_ci_lower * X_range\n",
    "    y_upper = intercept + beta_ci_upper * X_range\n",
    "    ax.fill_between(X_range, y_lower, y_upper, color='crimson', alpha=0.2, label='95% CI')\n",
    "\n",
    "    # Title and formatting\n",
    "    ax.set_title(f\"{title_prefix}{year} (R² = {model.rsquared:.3f})\", fontsize=22)\n",
    "    ax.set_xlabel(r'$\\log_{10}$(Population)', fontsize=24)\n",
    "    ax.set_ylabel(r'$\\log_{10}$(Cases)', fontsize=24)\n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "    ax.set_xlim(left=0)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax.legend(title=(\n",
    "        f\"β = {beta:.3f}; CI: [{beta_ci_lower:.3f}, {beta_ci_upper:.3f}]\\n\"\n",
    "        f\"γ = {intercept:.3f}; CI: [{intercept_ci_lower:.3f}, {intercept_ci_upper:.3f}]\"\n",
    "    ), fontsize=14, title_fontsize=16)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "plot_regression_scatter(axes[0], worst_year, df, title_prefix='Worst Year (Cumulative): ')\n",
    "plot_regression_scatter(axes[1], best_year, df, title_prefix='Best Year (Cumulative): ')\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\regressions\\temporal\\[2010-2024]regression_comparison_ts_counties.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "def plot_r2_timeseries(years, r2_values):\n",
    "    \"\"\"\n",
    "    Plots R² values over time as a time series.\n",
    "\n",
    "    Parameters:\n",
    "    - years: list of years (2000 to 2024)\n",
    "    - r2_values: list of R² values corresponding to each year\n",
    "    \"\"\"\n",
    "    r2_array = np.array(r2_values)\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(years, r2_array, marker='o', color='darkgreen', linewidth=3.5, label=r'$R^2$ value')\n",
    "    \n",
    "    # Highlight best and worst years\n",
    "    if np.any(~np.isnan(r2_array)):\n",
    "        best_idx = np.nanargmax(r2_array)\n",
    "        worst_idx = np.nanargmin(r2_array)\n",
    "\n",
    "        plt.scatter(years[best_idx], r2_array[best_idx], color='darkorange', s=100, label=f'Best R²: {years[best_idx]}')\n",
    "        plt.scatter(years[worst_idx], r2_array[worst_idx], color='crimson', s=100, label=f'Worst R²: {years[worst_idx]}')\n",
    "\n",
    "    plt.title(r'Time Series of $R^2$ Values (2010–2024) for $\\beta$ for Counties', fontsize=32)\n",
    "    plt.xlabel('Year', fontsize=28)\n",
    "    plt.ylabel(r'$R^2$ Value', fontsize=28)\n",
    "    plt.xticks(years, rotation=65, fontsize=24)\n",
    "    plt.yticks(fontsize=24)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(True, alpha=0.6)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\regressions\\temporal\\[2010-2024]r2_ts_counties.png', dpi=1200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_r2_timeseries(years, r2_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dcc318",
   "metadata": {},
   "source": [
    "##### Scaling of Total NamUs over Select Time Periods vs Population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import statsmodels.api as sm\n",
    "\n",
    "df_primary = pd.read_csv(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\mp_term.csv')\n",
    "df_primary['DisappearanceDate'] = pd.to_datetime(df_primary['DisappearanceDate'])\n",
    "\n",
    "df_primary = df_primary[\n",
    "    (df_primary['DisappearanceDate'] > pd.to_datetime('2009-12-31')) &\n",
    "    (df_primary['DisappearanceDate'] < pd.to_datetime('2025-01-01'))\n",
    "]\n",
    "\n",
    "df_msa = df_primary.groupby('MSA Code').agg(\n",
    "    Case_Count=('CaseID', 'count'),      # count the number of rows per MSA\n",
    "    MSA_Title=('MSA Title', 'first'),   # take the first MSA Title\n",
    "    CBSA_Type=('CBSA Type', 'first'),    # take the first CBSA Type (or whatever column you meant by 'MSA Type')\n",
    "    MSA_pop=('MSA_pop', 'last')\n",
    ").reset_index()\n",
    "\n",
    "# --- Clean and prepare data\n",
    "df_msa = df_msa[(df_msa['Case_Count'] > 0) & (df_msa['MSA_pop'] > 0)].dropna()\n",
    "df_msa['log_cases'] = np.log10(df_msa['Case_Count'])\n",
    "df_msa['log_pop'] = np.log10(df_msa['MSA_pop'])\n",
    "\n",
    "# Subsets\n",
    "datasets = {\n",
    "    'All CBSAs': df_msa,\n",
    "    'MSAs': df_msa[df_msa['CBSA_Type'] == 'MSA'],\n",
    "    'MicroSAs': df_msa[df_msa['CBSA_Type'] == 'MicroSA'],\n",
    "}\n",
    "\n",
    "# --- Plot setup\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8), sharey=True)\n",
    "\n",
    "for ax, (title, df) in zip(axes, datasets.items()):\n",
    "    # Fit log-log regression\n",
    "    X = sm.add_constant(df['log_pop'])\n",
    "    y = df['log_cases']\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    intercept, slope = model.params\n",
    "    conf_int = model.conf_int(alpha=0.05)\n",
    "    intercept_ci = conf_int.loc['const'].values\n",
    "    slope_ci = conf_int.loc['log_pop'].values\n",
    "    r2 = model.rsquared\n",
    "\n",
    "    # Line and confidence interval\n",
    "    x_vals = np.linspace(0, df['log_pop'].max(), 200)\n",
    "    x_vals_const = sm.add_constant(x_vals)\n",
    "    y_vals = model.predict(x_vals_const)\n",
    "    preds_ci = model.get_prediction(x_vals_const).summary_frame(alpha=0.05)\n",
    "\n",
    "    # Calculate mean and median points\n",
    "    mean_log_pop = df['log_pop'].mean()\n",
    "    mean_log_cases = df['log_cases'].mean()\n",
    "    median_log_pop = df['log_pop'].median()\n",
    "    median_log_cases = df['log_cases'].median()\n",
    "\n",
    "    # Plotting\n",
    "    ax.scatter(df['log_pop'], df['log_cases'], color='steelblue', alpha=0.7, label='$log_{10}$(Number of Cases per CBSA)')\n",
    "    ax.plot(x_vals, y_vals, color='darkred', linewidth=2, label='Regression line')\n",
    "    ax.fill_between(x_vals, preds_ci['mean_ci_lower'], preds_ci['mean_ci_upper'],\n",
    "                    color='lightcoral', alpha=0.3, label='95% CI band')\n",
    "\n",
    "    ax.scatter(mean_log_pop, mean_log_cases, color='green', s=100, edgecolor='black', label='Mean point', zorder=5)\n",
    "    ax.scatter(median_log_pop, median_log_cases, color='purple', s=100, edgecolor='black', label='Median point', zorder=5)\n",
    "\n",
    "    # Total cases as sum, not count\n",
    "    total_cases = df['Case_Count'].sum()\n",
    "    regression_label = (\n",
    "        f\"β = {slope:.3f} [{slope_ci[0]:.3f}, {slope_ci[1]:.3f}]\\n\"\n",
    "        f\"γ = {intercept:.3f} [{intercept_ci[0]:.3f}, {intercept_ci[1]:.3f}]\\n\"\n",
    "        f\"$R^2$ = {r2:.3f}\\n\"\n",
    "        f\"Total cases: {total_cases:,.0f}\"  # formatted with commas\n",
    "    )\n",
    "    ax.plot([], [], ' ', label=regression_label)\n",
    "\n",
    "    ax.set_title(title, fontsize=28)\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "    ax.legend(fontsize=12, loc='upper left')\n",
    "    ax.grid(True)\n",
    "    ax.set_xlim(left=0)\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "axes[0].set_ylabel('log(NamUS Case Counts)\\n[2010–2024]', fontsize=20)\n",
    "axes[0].set_xlabel('log(CBSA Population)', fontsize=20)\n",
    "axes[1].set_xlabel('log(MSA Population)', fontsize=20)\n",
    "axes[2].set_xlabel('log(MicroSA Population)', fontsize=20)\n",
    "\n",
    "fig.suptitle('Scaling Exponent (β) of Missing Persons Cases vs Population for CBSAs [2010–2024]', fontsize=28)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "fig.savefig(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\regressions\\[2010-2024]regressions.png', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2e9ca",
   "metadata": {},
   "source": [
    "### Demographic Plots:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d00a3a",
   "metadata": {},
   "source": [
    "#### CBSA Type Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_primary = pd.read_csv(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\mp_term.csv')\n",
    "df_primary['DisappearanceDate'] = pd.to_datetime(df_primary['DisappearanceDate'])\n",
    "\n",
    "df_primary = df_primary[\n",
    "    (df_primary['DisappearanceDate'] > pd.to_datetime('1968-12-31')) &\n",
    "    (df_primary['DisappearanceDate'] < pd.to_datetime('2025-01-01'))\n",
    "]\n",
    "\n",
    "def plot_cbsa_type_distribution(df):\n",
    "\n",
    "    if 'CBSA Type' not in df.columns:\n",
    "        raise ValueError(\"'CBSA Type' column not found in the DataFrame.\")\n",
    "    \n",
    "    # Count values, include NaNs under 'None'\n",
    "    counts = df['CBSA Type'].value_counts(dropna=False)\n",
    "    counts.index = counts.index.fillna('None')\n",
    "    total = counts.sum()\n",
    "\n",
    "    # Convert counts to DataFrame for sorting and indexing\n",
    "    counts_df = counts.sort_values(ascending=True).to_frame(name='count')\n",
    "    counts_df['percentage'] = counts_df['count'] / total * 100\n",
    "\n",
    "    # Use a color map for distinct colors\n",
    "    cmap = plt.get_cmap('tab20')  # Choose from: 'Set3', 'tab20', 'viridis', etc.\n",
    "    colors = [cmap(i) for i in range(len(counts_df))]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    bars = plt.bar(\n",
    "        counts_df.index,\n",
    "        counts_df['count'],\n",
    "        color=colors,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "\n",
    "    # Annotate each bar with count and percentage\n",
    "    for bar, count, perc in zip(bars, counts_df['count'], counts_df['percentage']):\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width() / 2,\n",
    "                height / 2,  # vertically centered inside the bar\n",
    "                f'{count:,}\\n({perc:.1f}%)',\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                fontsize=16,\n",
    "                color='black' if height > total * 0.05 else 'coral',  # ensure contrast\n",
    "                fontweight='bold'\n",
    "            )\n",
    "\n",
    "\n",
    "    plt.text(\n",
    "        0.05, 0.95,\n",
    "        f\"Total Cases: {total:,}\",\n",
    "        transform=plt.gca().transAxes,\n",
    "        fontsize=22,\n",
    "        ha='left',\n",
    "        va='top',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='black')\n",
    "    )\n",
    "    # Labels and formatting\n",
    "    plt.title(\n",
    "        f\"Distribution of NamUS Missing Persons Cases by CBSA Type (1969–2024)\",\n",
    "        fontsize=28\n",
    "    )\n",
    "    plt.xlabel(\"CBSA Type\", fontsize=28)\n",
    "    plt.ylabel(\"Number of Cases\", fontsize=28)\n",
    "    plt.xticks(rotation=45, fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(r\"F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\type_distribution\\[1969-2024]mp_type_distribution(cbsa).png\", dpi=1200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_cbsa_type_distribution(df_primary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60470426",
   "metadata": {},
   "source": [
    "#### Choropleth Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load data\n",
    "# --------------------------------------------------\n",
    "\n",
    "df_namus = pd.read_csv(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\mp_term.csv'\n",
    ")\n",
    "\n",
    "gdf_2024 = gpd.read_file(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\source\\shape files\\2024\\counties\\tl_2024_us_county.shp'\n",
    ")\n",
    "\n",
    "gdf_states_2024 = gpd.read_file(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\source\\shape files\\2024\\states\\tl_2024_us_state.shp'\n",
    ")\n",
    "\n",
    "gdf_2024['GEOID'] = gdf_2024['GEOID'].astype(str)\n",
    "gdf_2024['STATEFP'] = gdf_2024['STATEFP'].astype(str)\n",
    "gdf_states_2024['GEOID'] = gdf_states_2024['GEOID'].astype(str)\n",
    "gdf_states_2024['STATEFP'] = gdf_states_2024['STATEFP'].astype(str)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Keep only continental US (exclude AK, HI, PR)\n",
    "# --------------------------------------------------\n",
    "\n",
    "conus_states = {'02', '15', '72'}  # Alaska, Hawaii, Puerto Rico\n",
    "gdf_2024 = gdf_2024[~gdf_2024['STATEFP'].isin(conus_states)].copy()\n",
    "gdf_states_2024 = gdf_states_2024[~gdf_states_2024['STATEFP'].isin(conus_states)].copy()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Prepare case counts\n",
    "# --------------------------------------------------\n",
    "\n",
    "df_namus['FIPS'] = (\n",
    "    df_namus['FIPS']\n",
    "    .astype(str)\n",
    "    .str.zfill(5)\n",
    ")\n",
    "\n",
    "county_counts = (\n",
    "    df_namus\n",
    "    .groupby('FIPS')\n",
    "    .size()\n",
    "    .reset_index(name='case_count')\n",
    ")\n",
    "\n",
    "gdf = gdf_2024.merge(\n",
    "    county_counts,\n",
    "    left_on='GEOID',\n",
    "    right_on='FIPS',\n",
    "    how='left'\n",
    ")\n",
    "gdf['case_count'] = gdf['case_count'].fillna(0)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Log scaling (exclude zeros)\n",
    "# --------------------------------------------------\n",
    "\n",
    "vmin = gdf.loc[gdf['case_count'] > 0, 'case_count'].min()\n",
    "vmax = gdf['case_count'].max()\n",
    "\n",
    "norm = LogNorm(vmin=vmin, vmax=vmax)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Plot choropleth\n",
    "# --------------------------------------------------\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot map\n",
    "gdf.plot(\n",
    "    column='case_count',\n",
    "    ax=ax,\n",
    "    cmap='viridis',\n",
    "    linewidth=0.1,\n",
    "    edgecolor='gray',\n",
    "    norm=norm,\n",
    "    legend=False  # We'll add a horizontal colorbar manually\n",
    ")\n",
    "\n",
    "# Zoom to continental US bounds (approximate)\n",
    "ax.set_xlim([-125, -66])  # longitude\n",
    "ax.set_ylim([24, 50])     # latitude\n",
    "\n",
    "# Add horizontal colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)\n",
    "sm._A = []  # dummy array for ScalarMappable\n",
    "cbar = fig.colorbar(sm, ax=ax, orientation='horizontal', fraction=0.05, pad=0.05)\n",
    "cbar.set_label('$log_{10}$(Cumulative Missing Person Cases) (1969–2024)', fontsize=12)\n",
    "\n",
    "ax.set_title(\n",
    "    \"Cumulative NamUs Missing Person Cases by County (Continental U.S., 1969–2024)\",\n",
    "    fontsize=24,\n",
    "    fontweight='bold'\n",
    ")\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\demographics\\[1969-2024]_mp_county_choropleth.png',\n",
    "    dpi=1200,\n",
    "    bbox_inches='tight'\n",
    ")\n",
    "plt.show()\n",
    "####################################################################################################################\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Prepare case counts (STATE-LEVEL)\n",
    "# --------------------------------------------------\n",
    "\n",
    "df_namus['FIPS'] = (\n",
    "    df_namus['FIPS']\n",
    "    .astype(str)\n",
    "    .str.zfill(5)\n",
    ")\n",
    "\n",
    "# Extract state FIPS (first 2 digits)\n",
    "df_namus['STATEFP'] = df_namus['FIPS'].str[:2]\n",
    "\n",
    "state_counts = (\n",
    "    df_namus\n",
    "    .groupby('STATEFP')\n",
    "    .size()\n",
    "    .reset_index(name='case_count')\n",
    ")\n",
    "\n",
    "gdf = gdf_states_2024.merge(\n",
    "    state_counts,\n",
    "    on='STATEFP',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "gdf['case_count'] = gdf['case_count'].fillna(0)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Log scaling (exclude zeros)\n",
    "# --------------------------------------------------\n",
    "\n",
    "vmin = gdf.loc[gdf['case_count'] > 0, 'case_count'].min()\n",
    "vmax = gdf['case_count'].max()\n",
    "\n",
    "norm = LogNorm(vmin=vmin, vmax=vmax)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Plot choropleth\n",
    "# --------------------------------------------------\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "gdf.plot(\n",
    "    column='case_count',\n",
    "    ax=ax,\n",
    "    cmap='viridis',\n",
    "    linewidth=0.6,\n",
    "    edgecolor='gray',\n",
    "    norm=norm,\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Zoom to continental US bounds\n",
    "ax.set_xlim([-125, -66])\n",
    "ax.set_ylim([24, 50])\n",
    "\n",
    "# Horizontal colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)\n",
    "sm._A = []\n",
    "cbar = fig.colorbar(\n",
    "    sm,\n",
    "    ax=ax,\n",
    "    orientation='horizontal',\n",
    "    fraction=0.05,\n",
    "    pad=0.05\n",
    ")\n",
    "cbar.set_label(\n",
    "    '$log_{10}$(Cumulative Missing Person Cases) (1969–2024)',\n",
    "    fontsize=12\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    \"Cumulative NamUs Missing Person Cases by State \"\n",
    "    \"(Continental U.S., 1969–2024)\",\n",
    "    fontsize=24,\n",
    "    fontweight='bold'\n",
    ")\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\demographics\\[1969-2024]_mp_state_choropleth.png',\n",
    "    dpi=1200,\n",
    "    bbox_inches='tight'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de0695b",
   "metadata": {},
   "source": [
    "#### Pi Charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bf5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_namus = pd.read_csv(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\mp_term.csv'\n",
    ")\n",
    "\n",
    "df_namus['DisappearanceDate'] = pd.to_datetime(df_namus['DisappearanceDate'])\n",
    "\n",
    "df_namus = df_namus[\n",
    "    (df_namus['DisappearanceDate'] > pd.to_datetime('1968-12-31')) &\n",
    "    (df_namus['DisappearanceDate'] < pd.to_datetime('2025-01-01'))\n",
    "]\n",
    "\n",
    "df_namus['Sex'] = df_namus['Sex'].astype(str).str.strip().str.capitalize()\n",
    "df_namus['Ethnicity'] = df_namus['Ethnicity'].astype(str).str.strip()\n",
    "\n",
    "df_namus = df_namus.dropna(subset=['Sex', 'Ethnicity'])\n",
    "\n",
    "####################################################\n",
    "# Pi Chart of NamUs Missing Persons Cases\n",
    "####################################################\n",
    "sex_counts = df_namus['Sex'].value_counts()\n",
    "n_sex = sex_counts.sum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.pie(\n",
    "    sex_counts.values,\n",
    "    labels=sex_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    counterclock=False,\n",
    "    wedgeprops={'edgecolor': 'white'}\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    f\"Sex Distribution of Cumulative NamUs Missing Persons[1969-2024] Cases\\n(N = {n_sex:,} cases)\",\n",
    "    fontsize=14,\n",
    "    fontweight='bold'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\regressions\\demographics\\mp_sex_distribution.png',\n",
    "    dpi=1200,\n",
    "    bbox_inches='tight'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b40b3b",
   "metadata": {},
   "source": [
    "#### Population Pyraminds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb3d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "df_namus = pd.read_csv(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\mp_term.csv'\n",
    ")\n",
    "\n",
    "df_namus['DisappearanceDate'] = pd.to_datetime(df_namus['DisappearanceDate'])\n",
    "\n",
    "df_namus = df_namus[\n",
    "    (df_namus['DisappearanceDate'] > pd.to_datetime('1968-12-31')) &\n",
    "    (df_namus['DisappearanceDate'] < pd.to_datetime('2025-01-01'))\n",
    "]\n",
    "\n",
    "####################################################\n",
    "# Population Pyramid of Missing Persons\n",
    "####################################################\n",
    "\n",
    "# Keep only rows with valid age + sex\n",
    "df_plot = df_namus.dropna(subset=['CurrentMinAge', 'CurrentMaxAge', 'Sex']).copy()\n",
    "\n",
    "# Optional: standardize sex labels just in case\n",
    "df_plot['Sex'] = df_plot['Sex'].str.capitalize()\n",
    "\n",
    "# Number of included cases (IMPORTANT: from original rows)\n",
    "n_cases = df_plot.shape[0]\n",
    "\n",
    "age_bins = [\n",
    "    (0, 4), (5, 9), (10, 14), (15, 17), (18, 19), (20, 20), (21, 21),\n",
    "    (22, 24), (25, 29), (30, 34), (35, 39), (40, 44), (45, 49),\n",
    "    (50, 54), (55, 59), (60, 61), (62, 64), (65, 66), (67, 69),\n",
    "    (70, 74), (75, 79), (80, 84), (85, 120)\n",
    "]\n",
    "\n",
    "age_labels = [\n",
    "    'Under 5', '5-9', '10-14', '15-17', '18-19', '20', '21', '22-24',\n",
    "    '25-29', '30-34', '35-39', '40-44', '45-49', '50-54', '55-59',\n",
    "    '60-61', '62-64', '65-66', '67-69', '70-74', '75-79', '80-84', '85+'\n",
    "]\n",
    "\n",
    "def expand_to_age_bins(df):\n",
    "    records = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        for (low, high), label in zip(age_bins, age_labels):\n",
    "            if row['CurrentMaxAge'] >= low and row['CurrentMinAge'] <= high:\n",
    "                records.append({\n",
    "                    'Sex': row['Sex'],\n",
    "                    'AgeBin': label\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "expanded = expand_to_age_bins(df_plot)\n",
    "\n",
    "counts = (\n",
    "    expanded\n",
    "    .groupby(['AgeBin', 'Sex'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .reindex(age_labels)\n",
    ")\n",
    "\n",
    "# Ensure columns exist even if one sex is missing\n",
    "counts = counts.reindex(columns=['Male', 'Female'], fill_value=0)\n",
    "\n",
    "total = counts.sum().sum()\n",
    "male_percent = counts['Male'] / total * 100\n",
    "female_percent = counts['Female'] / total * 100\n",
    "\n",
    "y = np.arange(len(age_labels))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.barh(y, -male_percent.values, color='steelblue', label='Male')\n",
    "ax.barh(y,  female_percent.values, color='lightcoral', label='Female')\n",
    "\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(age_labels)\n",
    "ax.axvline(0, color='black', linewidth=1)\n",
    "\n",
    "# Make x-axis labels show positive percentages\n",
    "ax.xaxis.set_major_formatter(\n",
    "    mtick.FuncFormatter(lambda x, _: f\"{abs(x):.0f}%\")\n",
    ")\n",
    "\n",
    "# Symmetric x-limits\n",
    "max_val = max(male_percent.max(), female_percent.max())\n",
    "ax.set_xlim(-max_val * 1.1, max_val * 1.1)\n",
    "\n",
    "ax.set_xlabel(\"Percent of Total (%)\")\n",
    "ax.set_title(\"Age / Sex Distribution of Cumulative Missing Persons Cases [1969-2024]\")\n",
    "\n",
    "# Add N label (bottom-right)\n",
    "ax.text(\n",
    "    0.99, 0.01,\n",
    "    f\"N = {n_cases:,} cases\",\n",
    "    transform=ax.transAxes,\n",
    "    ha='right',\n",
    "    va='bottom',\n",
    "    fontsize=11,\n",
    "    color='gray'\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\regressions\\demographics\\population_pyramids\\[1969-2024]mp_pop_pyramid.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4daf9f8",
   "metadata": {},
   "source": [
    "#### Bar Charts of Ethnicity/Race Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b69e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_namus = pd.read_csv(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\export\\mp_term.csv'\n",
    ")\n",
    "\n",
    "df_namus['DisappearanceDate'] = pd.to_datetime(df_namus['DisappearanceDate'])\n",
    "\n",
    "df_namus = df_namus[\n",
    "    (df_namus['DisappearanceDate'] > pd.to_datetime('1968-12-31')) &\n",
    "    (df_namus['DisappearanceDate'] < pd.to_datetime('2025-01-01'))\n",
    "]\n",
    "\n",
    "df_namus['Sex'] = df_namus['Sex'].astype(str).str.strip().str.capitalize()\n",
    "df_namus['Ethnicity'] = df_namus['Ethnicity'].astype(str).str.strip()\n",
    "\n",
    "df_namus = df_namus.dropna(subset=['Sex', 'Ethnicity'])\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# BAR CHART: Ethnicity Distribution\n",
    "# ==================================================\n",
    "\n",
    "eth_counts = df_namus['Ethnicity'].value_counts()\n",
    "n_eth = eth_counts.sum()\n",
    "\n",
    "# Group small categories\n",
    "threshold = 0.03  # 3%\n",
    "eth_percent = eth_counts / n_eth\n",
    "small = eth_percent < threshold\n",
    "\n",
    "eth_counts_grouped = eth_counts.copy()\n",
    "if small.any():\n",
    "    eth_counts_grouped = eth_counts_grouped[~small]\n",
    "    eth_counts_grouped['Other'] = eth_counts[small].sum()\n",
    "\n",
    "# Convert to percentages\n",
    "eth_percent_grouped = eth_counts_grouped / n_eth * 100\n",
    "\n",
    "# Sort for nicer plotting\n",
    "eth_percent_grouped = eth_percent_grouped.sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars = ax.barh(\n",
    "    eth_percent_grouped.index,\n",
    "    eth_percent_grouped.values,\n",
    "    color='slategray',\n",
    "    edgecolor='black',\n",
    "    alpha=0.85\n",
    ")\n",
    "\n",
    "# Label bars with % and count\n",
    "for bar, label in zip(bars, eth_percent_grouped.index):\n",
    "    pct = eth_percent_grouped[label]\n",
    "    count = eth_counts_grouped[label]\n",
    "    ax.text(\n",
    "        bar.get_width() + 0.3,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{pct:.1f}% (N={count:,})\",\n",
    "        va='center',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Percent of Total (%)\")\n",
    "ax.set_title(\n",
    "    f\"Ethnicity Distribution of Cumulative NamUs Missing Persons[1969-2024] Cases\\n(N = {n_eth:,} cases)\",\n",
    "    fontsize=14,\n",
    "    fontweight='bold'\n",
    ")\n",
    "\n",
    "ax.set_xlim(0, eth_percent_grouped.max() * 1.25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\regressions\\demographics\\mp_ethnicity_bar.png',\n",
    "    dpi=1200,\n",
    "    bbox_inches='tight'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389a868",
   "metadata": {},
   "source": [
    "## Mexico INEGI Visuals:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00be09d",
   "metadata": {},
   "source": [
    "### Scaling per 100k [2006-2025]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "This script creates a bar chart showing the trend of the rate of\n",
    "missing people for the specified state in Mexico.\n",
    "\n",
    "It is very easy to use, you only need to run the main function with\n",
    "one parameter: the ID of the state.\n",
    "\n",
    "The missing people data was sourced from:\n",
    "\n",
    "https://consultapublicarnpdno.segob.gob.mx/consulta\n",
    "\n",
    "The original data was cleaned, translated and anonymized.\n",
    "\n",
    "The population data was sourced from:\n",
    "\n",
    "https://datos.gob.mx/busca/dataset/proyecciones-de-la-poblacion-de-mexico-y-de-las-entidades-federativas-2020-2070/resource/ae83f2b0-f23e-45e3-91ae-f85594775dff\n",
    "\n",
    "The population dataset included in this project is meant to only\n",
    "be used within the project, as it had some adjustments done (removal of\n",
    "columns and translation).\n",
    "\n",
    "This project uses the free Montserrat font -> https://fonts.google.com/specimen/Montserrat\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# Each state_id has a corresponding name.\n",
    "# 0 is for national level figures.\n",
    "STATES = {\n",
    "    0: \"Mexico\",\n",
    "    1: \"Aguascalientes\",\n",
    "    2: \"Baja California\",\n",
    "    3: \"Baja California Sur\",\n",
    "    4: \"Campeche\",\n",
    "    5: \"Coahuila\",\n",
    "    6: \"Colima\",\n",
    "    7: \"Chiapas\",\n",
    "    8: \"Chihuahua\",\n",
    "    9: \"Ciudad de México\",\n",
    "    10: \"Durango\",\n",
    "    11: \"Guanajuato\",\n",
    "    12: \"Guerrero\",\n",
    "    13: \"Hidalgo\",\n",
    "    14: \"Jalisco\",\n",
    "    15: \"Estado de México\",\n",
    "    16: \"Michoacán\",\n",
    "    17: \"Morelos\",\n",
    "    18: \"Nayarit\",\n",
    "    19: \"Nuevo León\",\n",
    "    20: \"Oaxaca\",\n",
    "    21: \"Puebla\",\n",
    "    22: \"Querétaro\",\n",
    "    23: \"Quintana Roo\",\n",
    "    24: \"San Luis Potosí\",\n",
    "    25: \"Sinaloa\",\n",
    "    26: \"Sonora\",\n",
    "    27: \"Tabasco\",\n",
    "    28: \"Tamaulipas\",\n",
    "    29: \"Tlaxcala\",\n",
    "    30: \"Veracruz\",\n",
    "    31: \"Yucatán\",\n",
    "    32: \"Zacatecas\",\n",
    "}\n",
    "\n",
    "\n",
    "def main(state_id):\n",
    "    \"\"\"\n",
    "    Creates a bar chart with the yearly rate of missing people\n",
    "    for the specified state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state_id : int\n",
    "        The ID of the state you want to plot.\n",
    "        Use 0 for national figures.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # We load the population dataset.\n",
    "    pop = pd.read_csv(r\"F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\source\\mexico_missing_persons\\population.csv\")\n",
    "\n",
    "    # We filter by the specified state_id.\n",
    "    pop = pop[pop[\"STATE_ID\"] == state_id]\n",
    "\n",
    "    # We calculate the total population by year.\n",
    "    pop = pop.groupby(\"YEAR\").sum(numeric_only=True)\n",
    "\n",
    "    # We load the missing people dataset.\n",
    "    df = pd.read_csv(r\"F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\source\\mexico_missing_persons\\data.csv\")\n",
    "\n",
    "    # A victim can have multiple reports, the first thing to do\n",
    "    # is to only select one record per victim.\n",
    "    df = df.groupby(\"VICTIM_ID\").last()\n",
    "\n",
    "    # We filter by state. If it is 0 we skip this step as\n",
    "    # 0 is for national level figures.\n",
    "    if state_id != 0:\n",
    "        df = df[df[\"STATE_ID\"] == state_id]\n",
    "\n",
    "    # We will convert to datetime the date of incidence and date of report columns.\n",
    "    df[\"DATE_OF_INCIDENCE\"] = pd.to_datetime(df[\"DATE_OF_INCIDENCE\"], errors=\"coerce\")\n",
    "    df[\"DATE_OF_REPORT\"] = pd.to_datetime(df[\"DATE_OF_REPORT\"], errors=\"coerce\")\n",
    "\n",
    "    # To get the counts by year we will prefer the date of incidence\n",
    "    # but we don't always have it. In that case we fallback to the date of report.\n",
    "    df[\"DATE_OF_INCIDENCE\"] = df[\"DATE_OF_INCIDENCE\"].fillna(df[\"DATE_OF_REPORT\"])\n",
    "\n",
    "    # Now we calculate the yearly counts.\n",
    "    df = df[\"DATE_OF_INCIDENCE\"].value_counts().resample(\"YS\").sum().to_frame(\"total\")\n",
    "\n",
    "    # We only need the year from the index.\n",
    "    df.index = df.index.year\n",
    "\n",
    "    # We add the population to our missing people DataFrame.\n",
    "    df[\"pop\"] = pop[\"POPULATION\"]\n",
    "\n",
    "    # We calculate the rate per 100,000 inhabitants.\n",
    "    df[\"rate\"] = df[\"total\"] / df[\"pop\"] * 100000\n",
    "\n",
    "    # We only select the latest 20 years.\n",
    "    df = df.tail(20)\n",
    "\n",
    "    # We create the text for each bar.\n",
    "    df[\"text\"] = df.apply(\n",
    "        lambda x: f\"<b>{x['rate']:,.2f}</b><br>({x['total']:,.0f})\", axis=1\n",
    "    )\n",
    "\n",
    "    # We will create a simple bar chart with all the previous calculations.\n",
    "    # The bar chart will have a color scale from yellow (0) to red (max).\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df.index,\n",
    "            y=df[\"rate\"],\n",
    "            text=df[\"text\"],\n",
    "            name=f\"Cummulative total: <b>{df['total'].sum():,.0f}</b> actively missing.<br>Doesn't include confidential records.\",\n",
    "            textposition=\"outside\",\n",
    "            marker_color=df[\"rate\"],\n",
    "            marker_colorscale=\"portland\",\n",
    "            marker_cmid=0,\n",
    "            marker_line_width=0,\n",
    "            textfont_size=30,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        ticks=\"outside\",\n",
    "        ticklen=10,\n",
    "        zeroline=False,\n",
    "        tickcolor=\"#FFFFFF\",\n",
    "        linewidth=2,\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        gridwidth=0.35,\n",
    "        mirror=True,\n",
    "        nticks=len(df) + 1,\n",
    "    )\n",
    "\n",
    "    # The y-axis range is dinamically set to make room for the tallest bar text.\n",
    "    fig.update_yaxes(\n",
    "        title=\"Rate per 100,000 inhabitants\",\n",
    "        range=[0, df[\"rate\"].max() * 1.1],\n",
    "        ticks=\"outside\",\n",
    "        separatethousands=True,\n",
    "        tickfont_size=14,\n",
    "        ticklen=10,\n",
    "        title_standoff=15,\n",
    "        tickcolor=\"#FFFFFF\",\n",
    "        linewidth=2,\n",
    "        gridwidth=0.35,\n",
    "        showline=True,\n",
    "        nticks=20,\n",
    "        zeroline=False,\n",
    "        mirror=True,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        showlegend=True,\n",
    "        legend_borderwidth=1,\n",
    "        legend_bordercolor=\"#FFFFFF\",\n",
    "        legend_x=0.01,\n",
    "        legend_y=0.98,\n",
    "        legend_xanchor=\"left\",\n",
    "        legend_yanchor=\"top\",\n",
    "        width=1920,\n",
    "        height=1080,\n",
    "        font_family=\"Montserrat\",\n",
    "        font_color=\"#FFFFFF\",\n",
    "        font_size=24,\n",
    "        title_text=f\"Evolution of the rate of missing and unaccounted-for people in <b>{STATES[state_id]}</b> ({df.index.min()}-{df.index.max()})\",\n",
    "        title_x=0.5,\n",
    "        title_y=0.965,\n",
    "        margin_t=80,\n",
    "        margin_r=40,\n",
    "        margin_b=120,\n",
    "        margin_l=130,\n",
    "        title_font_size=34,\n",
    "        paper_bgcolor=\"#2B2B2B\",\n",
    "        plot_bgcolor=\"#171010\",\n",
    "        annotations=[\n",
    "            dict(\n",
    "                x=0.01,\n",
    "                y=-0.11,\n",
    "                xref=\"paper\",\n",
    "                yref=\"paper\",\n",
    "                xanchor=\"left\",\n",
    "                yanchor=\"top\",\n",
    "                text=\"Source: RNPDNO (July 2025)\",\n",
    "            ),\n",
    "            dict(\n",
    "                x=0.5,\n",
    "                y=-0.11,\n",
    "                xref=\"paper\",\n",
    "                yref=\"paper\",\n",
    "                xanchor=\"center\",\n",
    "                yanchor=\"top\",\n",
    "                text=\"Year of incidence\",\n",
    "            ),\n",
    "            dict(\n",
    "                x=1.01,\n",
    "                y=-0.11,\n",
    "                xref=\"paper\",\n",
    "                yref=\"paper\",\n",
    "                xanchor=\"right\",\n",
    "                yanchor=\"top\",\n",
    "                text=\"🧁 @lapanquecita\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # We name the resulting figure with the state_id.\n",
    "    fig.write_image(fr\"F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\mexico\\{state_id}.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f38bf",
   "metadata": {},
   "source": [
    "### INEGI Age & Sex Demographic Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Example: df already exists\n",
    "df_inegi = pd.read_csv(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\source\\mexico_missing_persons\\data.csv', dtype=str)\n",
    "\n",
    "# Ensure DATE_OF_BIRTH is datetime\n",
    "df_inegi[\"DATE_OF_BIRTH\"] = pd.to_datetime(df_inegi[\"DATE_OF_BIRTH\"], errors=\"coerce\")\n",
    "df_inegi[\"DATE_OF_INCIDENCE\"] = pd.to_datetime(df_inegi[\"DATE_OF_INCIDENCE\"], errors=\"coerce\")\n",
    "\n",
    "# -----------------------\n",
    "# PIE CHART: SEX\n",
    "# -----------------------\n",
    "# Normalize SEX values and keep special categories\n",
    "df_inegi[\"SEX_CLEAN\"] = (\n",
    "    df_inegi[\"SEX\"]\n",
    "    .fillna(\"MISSING\")\n",
    "    .str.upper()\n",
    ")\n",
    "\n",
    "# Count values\n",
    "sex_counts = df_inegi[\"SEX_CLEAN\"].value_counts()\n",
    "\n",
    "# Desired order\n",
    "desired_order = [\"MISSING\", \"CONFIDENTIAL\", \"MALE\", \"FEMALE\"]\n",
    "\n",
    "# Reindex to enforce order and keep existing categories\n",
    "sex_counts = sex_counts.reindex(\n",
    "    [x for x in desired_order if x in sex_counts.index]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "def autopct_with_counts(values):\n",
    "    def inner(pct):\n",
    "        total = sum(values)\n",
    "        count = int(round(pct * total / 100.0))\n",
    "        return f\"{pct:.1f}%\\n(n={count})\"\n",
    "    return inner\n",
    "\n",
    "wedges, _, _ = plt.pie(\n",
    "    sex_counts,\n",
    "    autopct=autopct_with_counts(sex_counts),\n",
    "    startangle=0  # x = 0\n",
    ")\n",
    "\n",
    "plt.title(\"Sex Distribution of INEGI Missing Persons Cases\")\n",
    "\n",
    "# Legend at the bottom with counts\n",
    "legend_labels = [f\"{sex} (n={count})\" for sex, count in sex_counts.items()]\n",
    "plt.legend(\n",
    "    wedges,\n",
    "    legend_labels,\n",
    "    title=\"Sex\",\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, -0.15),\n",
    "    ncol=2\n",
    ")\n",
    "\n",
    "plt.axis(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\mexico\\INEGI_sex_pi_chart.png')\n",
    "plt.show()\n",
    "\n",
    "# -----------------------\n",
    "# BAR CHART: AGE BRACKETS\n",
    "# -----------------------\n",
    "today = pd.Timestamp(datetime.today())\n",
    "df_inegi[\"AGE_MISSING\"] = ((df_inegi['DATE_OF_INCIDENCE'] - df_inegi[\"DATE_OF_BIRTH\"]).dt.days // 365)\n",
    "\n",
    "bins = [0, 10, 20, 30, 40, 50, 60, 70, 200]\n",
    "labels = [\"0–9\", \"10–19\", \"20–29\", \"30–39\",\n",
    "          \"40–49\", \"50–59\", \"60–69\", \"70+\"]\n",
    "\n",
    "df_inegi[\"AGE_BRACKET\"] = pd.cut(df_inegi[\"AGE_MISSING\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "age_counts = df_inegi[\"AGE_BRACKET\"].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(age_counts.index.astype(str), age_counts.values)\n",
    "\n",
    "plt.xlabel(\"Age Bracket\")\n",
    "plt.ylabel(\"Number of Cases\")\n",
    "plt.title(\"Distribution of INEGI Missing Persons Cases: Ages at Incidence\")\n",
    "\n",
    "# Add count labels above bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        height,\n",
    "        f\"{int(height)}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'F:\\dsl_CLIMA\\projects\\submittable\\missing persons\\plots\\mexico\\INEGI_age_at_incidence_barChart.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a052b5",
   "metadata": {},
   "source": [
    "# Missing Persons & Population Data Sources\n",
    "\n",
    "---\n",
    "\n",
    "## United States Missing Persons & Population Data\n",
    "\n",
    "### Census & Population Data\n",
    "- **University of Brown’s Guide to Census Data**  \n",
    "  https://libguides.brown.edu/census/histmicro\n",
    "\n",
    "- **US Census Population Estimates by County (1969–2022)**  \n",
    "  https://seer.cancer.gov/popdata.thru.2022/download.html\n",
    "\n",
    "- **SEER FIPS Updates (1969–2022)**  \n",
    "  https://seer.cancer.gov/seerstat/variables/countyattribs/time-dependent.html\n",
    "\n",
    "- **David Dorn FIPS Updates (1980–2021)**  \n",
    "  https://www.ddorn.net/data/FIPS_County_Code_Changes.pdf\n",
    "\n",
    "- **NHGIS – Historical Shapefiles**  \n",
    "  https://data2.nhgis.org/main\n",
    "\n",
    "---\n",
    "\n",
    "### Missing Persons Scrapers & Tools\n",
    "\n",
    "#### NightOwlRecon (NamUS Data)\n",
    "- **GitHub Repository**  \n",
    "  https://github.com/NightOwlRecon/NamUs-Data/blob/main/namus.py\n",
    "\n",
    "- **Extracted `.json` Dataset**  \n",
    "  https://drive.google.com/file/d/1k8PRzRlwE_Ti52enW4qjNX0bInr5Pf0g/view?usp=sharing\n",
    "\n",
    "#### Prepager (NamUS Scraper)\n",
    "- **GitHub Repository**  \n",
    "  https://github.com/Prepager/namus-scraper\n",
    "\n",
    "---\n",
    "\n",
    "### Crosswalks & Spatial References\n",
    "- **Historical County / MSA / CSA Crosswalks (BLS)**  \n",
    "  https://www.bls.gov/cew/classifications/areas/county-msa-csa-crosswalk.htm\n",
    "\n",
    "- **Connecticut Town Crosswalks (2023–Present)**  \n",
    "  https://data.ct.gov/Local-Government/Connecticut-Towns-Crosswalk-with-Tax-Codes-and-FIP/5hqs-h5c3/about_data\n",
    "\n",
    "- **Connecticut FIPS Codes for Planning Regions (AP Elections API)**  \n",
    "  https://developer.ap.org/ap-elections-api/docs/CT_FIPS_Codes_forPlanningRegions.htm\n",
    "\n",
    "---\n",
    "\n",
    "## South American & Latin American Missing Persons Data\n",
    "\n",
    "### Mexico\n",
    "- **INEGI Missing & Unaccounted-for Persons Dataset**  \n",
    "  https://figshare.com/articles/dataset/Missing_and_Unaccounted-for_People_in_Mexico_1960s_2025_/28283000\n",
    "\n",
    "- **Governmental Crime & Missing Persons Report**  \n",
    "  https://www.gob.mx/sesnsp/acciones-y-programas/incidencia-delictiva-del-fuero-comun-nueva-metodologia\n",
    "\n",
    "### New Mexico\n",
    "- **INEGI Missing Persons (ELCRI)**  \n",
    "  https://elcri.men/en/about/\n",
    "\n",
    "### Colombia\n",
    "- **SIEDCO Missing Persons Statistics (National Police)**  \n",
    "  https://www.policia.gov.co/estadistica-delictiva\n",
    "\n",
    "### Argentina\n",
    "- **Personas Perdidas**  \n",
    "  http://personasperdidas.org.ar/looking_for_their_families\n",
    "\n",
    "- **Police Reports (2020–2024)**  \n",
    "  https://www.datos.gob.ar/dataset/justicia-lucha-contra-trata-personas---llamados-linea-145---denuncias/archivo/justicia_4b786057-973f-4bd6-9594-8b74233ad9b1\n",
    "\n",
    "- **Federal System for Searching for Missing and Lost Persons – Management Report**  \n",
    "  https://www.argentina.gob.ar/sites/default/files/ministerio-seguridad-argentina-informe-gestion-sifebu-2024.pdf\n",
    "\n",
    "### Brazil\n",
    "- **Missing Persons & Forced Disappearances (Academic Article)**  \n",
    "  https://www.sciencedirect.com/science/article/pii/S2665910722000330\n",
    "\n",
    "---\n",
    "\n",
    "## Anecdotal / Qualitative Data & Additional Resources\n",
    "\n",
    "- **The Charley Project – Geographic Case Search (Mexico)**  \n",
    "  https://charleyproject.org/case-searches/geographical-cases?region=Mexico\n",
    "\n",
    "- **The Doe Network**  \n",
    "  https://www.doenetwork.org/\n",
    "\n",
    "- **The Lost People – NamUS Choropleth Mapping**  \n",
    "  https://jseibel55.github.io/The-Lost-People/#collapseThree\n",
    "\n",
    "- **A Consistent County-Level Spatial Crosswalk Since 1790**  \n",
    "  https://fpeckert.me/papers/egp-spatialcrosswalk.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
